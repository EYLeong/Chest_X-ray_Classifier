{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pillow\n",
    "from PIL import Image\n",
    "# Torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "#OS\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lung_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lung Dataset Consisting of Infected and Non-Infected.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, purpose, verbose=0):\n",
    "        \"\"\"\n",
    "        Constructor for generic Dataset class - simply assembles\n",
    "        the important parameters in attributes.\n",
    "        \n",
    "        Parameter:\n",
    "        -purpose variable should be set to a string of either 'train', 'test' or 'val'\n",
    "        -verbose takes an int of either 0,1 or 2. 0 will only differentiate between normal and infected, 1 will differentiate\n",
    "            between normal, covid and non-covid while 2 will only differentiate between covid and non-covid\n",
    "        \"\"\"\n",
    "        self.purpose = purpose\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # All images are of size 150 x 150\n",
    "        self.img_size = (150, 150)\n",
    "            \n",
    "        # The dataset has been split in training, testing and validation datasets\n",
    "        self.groups = ['train', 'test', 'val']\n",
    "        \n",
    "        # Path to images for different parts of the dataset\n",
    "        self.dataset_paths = {'train_normal': './dataset/train/normal/',\n",
    "                              'train_infected': './dataset/train/infected/',\n",
    "                              'train_infected_covid': './dataset/train/infected/covid',\n",
    "                              'train_infected_non_covid': './dataset/train/infected/non-covid',\n",
    "                              'test_normal': './dataset/test/normal/',\n",
    "                              'test_infected': './dataset/test/infected/',\n",
    "                              'test_infected_covid': './dataset/test/infected/covid',\n",
    "                              'test_infected_non_covid': './dataset/test/infected/non-covid',\n",
    "                              'val_normal': './dataset/val/normal/',\n",
    "                              'val_infected': './dataset/val/infected/',\n",
    "                              'val_infected_covid': './dataset/val/infected/covid',\n",
    "                              'val_infected_non_covid': './dataset/val/infected/non-covid'}\n",
    "        \n",
    "        #Contains the number of images that will be used in our dataset. To be populated below\n",
    "        self.dataset_numbers = {}\n",
    "        \n",
    "        # Consider normal and infected only\n",
    "        if verbose == 0:\n",
    "            self.classes = {0: 'normal', 1: 'infected'}\n",
    "            \n",
    "            #Populate self.dataset_numbers\n",
    "            for condition in self.classes.values():\n",
    "                #Key that will be used to access the dictionary\n",
    "                key = \"{}_{}\".format(self.purpose, condition)\n",
    "                if condition == \"normal\":\n",
    "                    #Retrieve the filepath and populate dataset_numbers with the number of images in that folder\n",
    "                    file_path = self.dataset_paths[key]\n",
    "                    count = len(os.listdir(file_path))\n",
    "                    self.dataset_numbers[key] = count\n",
    "                    \n",
    "                #For the infected case, we will be combining the covid and non-covid images into 1 category\n",
    "                else:\n",
    "                    #Need the keys for both covid and non-covid\n",
    "                    key1 = key + \"_covid\"\n",
    "                    key2 = key + \"_non_covid\"\n",
    "                    file_path1 = self.dataset_paths[key1]\n",
    "                    file_path2 = self.dataset_paths[key2]\n",
    "                    count1 = len(os.listdir(file_path1))\n",
    "                    count2 = len(os.listdir(file_path2))\n",
    "                    #Number of infected images will be number of covid + number of non-covid\n",
    "                    count = count1 + count2\n",
    "                    self.dataset_numbers[key] = count\n",
    "                       \n",
    "        #Consider normal, covid and non-covid\n",
    "        elif verbose == 1:\n",
    "            self.classes = {0: 'normal', 1: 'covid', 2: 'non_covid'}\n",
    "        \n",
    "            #Populate self.dataset_numbers\n",
    "            for condition in self.classes.values():\n",
    "                #Similar to verbose == 0 above\n",
    "                if condition == \"normal\":\n",
    "                    key = \"{}_{}\".format(self.purpose, condition)\n",
    "                    file_path = self.dataset_paths[key]\n",
    "                    count = len(os.listdir(file_path))\n",
    "                    self.dataset_numbers[key] = count\n",
    "                    \n",
    "                #For the infected case, we will be considering the covid and non-covid separately\n",
    "                else:\n",
    "                    #key = {purpose}_infected_covid or key = {purpose})_infected_non_covid\n",
    "                    key = \"{}_infected\".format(self.purpose)\n",
    "                    #Obtain the respective keys, retrieve the filepaths, populate dataset_numbers accordingly\n",
    "                    key1 = key + \"_covid\"\n",
    "                    key2 = key + \"_non_covid\"\n",
    "                    file_path1 = self.dataset_paths[key1]\n",
    "                    file_path2 = self.dataset_paths[key2]\n",
    "                    count1 = len(os.listdir(file_path1))\n",
    "                    count2 = len(os.listdir(file_path2))\n",
    "                    self.dataset_numbers[key1] = count1\n",
    "                    self.dataset_numbers[key2] = count2\n",
    "                \n",
    "        #Consider covid and non-covid\n",
    "        elif verbose == 2:\n",
    "            self.classes = {0: 'covid', 1 :'non_covid' }\n",
    "\n",
    "            #Populate self.dataset_numbers\n",
    "            for condition in self.classes.values():\n",
    "                #Similar to the infected case in verbose 2\n",
    "                key = \"{}_infected\".format(self.purpose)\n",
    "                key1 = key + \"_covid\"\n",
    "                key2 = key + \"_non_covid\"\n",
    "                file_path1 = self.dataset_paths[key1]\n",
    "                file_path2 = self.dataset_paths[key2]\n",
    "                count1 = len(os.listdir(file_path1))\n",
    "                count2 = len(os.listdir(file_path2))\n",
    "                self.dataset_numbers[key1] = count1\n",
    "                self.dataset_numbers[key2] = count2\n",
    "            \n",
    "        else:\n",
    "            err_msg  = \"Verbose argument only takes in an int of either 0,1 or 2\"\n",
    "            raise TypeError(err_msg)\n",
    "        \n",
    "        \n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        Descriptor function.\n",
    "        Will print details about the dataset when called.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate description\n",
    "        msg = \"This is the Lung {} Dataset in the 50.039 Deep Learning class project\".format(self.purpose)\n",
    "        msg += \" in Feb-March 2021. \\n\"\n",
    "        msg += \"It contains a total of {} images, \".format(sum(self.dataset_numbers.values()))\n",
    "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
    "        msg += \"The images are stored in the following locations \"\n",
    "        msg += \"and each one contains the following number of images:\\n\"\n",
    "        for key, val in self.dataset_numbers.items():\n",
    "            file_path = self.dataset_paths[key]\n",
    "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, file_path, val)\n",
    "        print(msg)\n",
    "        \n",
    "        \n",
    "    def open_img(self, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - class_val variable should be set to 'normal' or 'infected'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \n",
    "        Returns loaded image as a normalized Numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Error handling 1\n",
    "        group_val = self.purpose\n",
    "        err_msg = \"Error - For verbose = {}, class_val variable should be set to \".format(self.verbose)\n",
    "        count = 0\n",
    "        for value in self.classes.values():\n",
    "            add_on = \"'\" + value + \"'\"\n",
    "            if count < (len(self.classes.values()) -1):\n",
    "                add_on += ' or '\n",
    "            err_msg += add_on\n",
    "            count += 1\n",
    "        assert class_val in self.classes.values(), err_msg\n",
    "        \n",
    "        #For covid and non_covid, we want the class val to be \"infected_covid\" or \"infected_non_covid\"\n",
    "        if class_val == 'covid' or class_val == 'non_covid':\n",
    "            class_val = 'infected_' + class_val\n",
    "            \n",
    "        #Error Handling 2\n",
    "        #Retrieve the max_val from self.dataset_numbers using the respective keys\n",
    "        max_val = self.dataset_numbers['{}_{}'.format(group_val, class_val)]\n",
    "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
    "        class_val_err = class_val.replace('_', '/')\n",
    "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(group_val, class_val_err, max_val)\n",
    "        assert isinstance(index_val, int), err_msg\n",
    "        assert index_val >= 0 and index_val <= max_val, err_msg\n",
    "        \n",
    "        #Retrieve the image file path\n",
    "        if class_val != \"infected\":\n",
    "            #path_to_file will be the filepath/index_val \n",
    "            #Filepath is the path to the dataset folder as stored in self.dataset_paths\n",
    "            #index_val is the image number that we want to retrieve\n",
    "            path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}'.format(group_val, class_val)], index_val)\n",
    "        \n",
    "        #If the class_val == infected, we need to take into account for both covid and non_covid images as they both\n",
    "        #make up the infected class\n",
    "        else:\n",
    "            #Retrieve the number of images in the covid folder\n",
    "            covid_count = len(os.listdir(self.dataset_paths['{}_{}_covid'.format(group_val, class_val)]))\n",
    "            \n",
    "            #If the infected image number we want to retrieve is smaller than the max image number in the covid folder,\n",
    "            #We retrieve the infected image from the covid folder\n",
    "            if index_val < covid_count:\n",
    "                path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}_covid'.format(group_val, class_val)], index_val)\n",
    "            \n",
    "            #Else if it is greater, we move on to the non_covid folder and retrieve the image from there\n",
    "            else:\n",
    "                index_val = index_val - covid_count #remember that the image index starts from 0, so we got to reset the index\n",
    "                path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_{}_non_covid'.format(group_val, class_val)], index_val)\n",
    "        \n",
    "        #Retrieve the image using the file path\n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            im = np.asarray(Image.open(f))/255 #normalize the pixel values to be in the range of [0,1]\n",
    "        f.close()\n",
    "        return im\n",
    "    \n",
    "    def show_img(self, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens, then displays image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - class_val variable should be set to 'normal' or 'infected'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \"\"\"\n",
    "        # Open image\n",
    "        im = self.open_img(class_val, index_val)\n",
    "        \n",
    "        # Display\n",
    "        plt.imshow(im)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length special method, returns the number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Length function\n",
    "        return sum(self.dataset_numbers.values())\n",
    "    \n",
    "    def length(self):\n",
    "        return sum(self.dataset_numbers.values())\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Getitem special method.\n",
    "        \n",
    "        Expects an integer value index, between 0 and len(self) - 1.\n",
    "        \n",
    "        Returns the image and its label as a one hot vector, both\n",
    "        in torch tensor format in dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        #If we only have 2 classes\n",
    "        if self.verbose == 0 or self.verbose == 2:\n",
    "            #Get the total number of images belonging to the first class\n",
    "            first_val = int(list(self.dataset_numbers.values())[0])\n",
    "            #As long as the index is smaller thatn the total, this image belongs to that class\n",
    "            if index < first_val:\n",
    "                class_val = self.classes[0]\n",
    "                label = torch.Tensor([1, 0])\n",
    "                \n",
    "            #Else, we move on to the next class\n",
    "            else:\n",
    "                class_val = self.classes[1]\n",
    "                index = index - first_val #Remember to reset the index\n",
    "                label = torch.Tensor([0, 1])\n",
    "            im = self.open_img(class_val, index)\n",
    "            im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "          \n",
    "        #If we have 3 classes to consider\n",
    "        elif self.verbose == 1:\n",
    "            #Similar to the 2 class problem, just that we have 3 classes to consider now\n",
    "            first_val = int(list(self.dataset_numbers.values())[0]) #total number belonging to first class\n",
    "            second_val = int(list(self.dataset_numbers.values())[1]) #total number belonging to second class\n",
    "            \n",
    "            #First class\n",
    "            if index < first_val:\n",
    "                class_val = self.classes[0]\n",
    "                label = torch.Tensor([1, 0, 0])\n",
    "            \n",
    "            #Second class\n",
    "            elif index >= first_val and index < first_val + second_val:\n",
    "                index = index - first_val #Reset Index\n",
    "                class_val = self.classes[1]\n",
    "                label = torch.Tensor([0,1,0])\n",
    "            \n",
    "            #Third class\n",
    "            else:\n",
    "                index = index-(first_val + second_val) #Reset index\n",
    "                class_val = self.classes[2]\n",
    "                label = torch.Tensor([0,0,1])\n",
    "            im = self.open_img(class_val, index)\n",
    "            im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "                \n",
    "        else:\n",
    "            raise TypeError(\"Verbose value is not 0,1 or 2\")\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Lung test Dataset in the 50.039 Deep Learning class project in Feb-March 2021. \n",
      "It contains a total of 615 images, of size 150 by 150.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - test_normal, in folder ./dataset/test/normal/: 234 images.\n",
      " - test_infected, in folder ./dataset/test/infected/: 381 images.\n",
      "\n",
      "{'test_normal': 234, 'test_infected': 381}\n"
     ]
    }
   ],
   "source": [
    "ld_train = Lung_Dataset('test', verbose =0)\n",
    "ld_train.describe()\n",
    "print(ld_train.dataset_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615\n"
     ]
    }
   ],
   "source": [
    "print(len(ld_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_normal': 234, 'test_infected': 381}\n"
     ]
    }
   ],
   "source": [
    "print(ld_train.dataset_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 150, 150])\n",
      "tensor([[[0.1373, 0.1647, 0.1961,  ..., 0.1569, 0.1294, 0.1059],\n",
      "         [0.1255, 0.1569, 0.1882,  ..., 0.1529, 0.1255, 0.0980],\n",
      "         [0.1176, 0.1451, 0.1804,  ..., 0.1529, 0.1216, 0.0941],\n",
      "         ...,\n",
      "         [0.0627, 0.0667, 0.0627,  ..., 0.0941, 0.0941, 0.0980],\n",
      "         [0.0627, 0.0667, 0.0627,  ..., 0.0941, 0.0941, 0.0980],\n",
      "         [0.0627, 0.0667, 0.0627,  ..., 0.0941, 0.0941, 0.0980]]])\n",
      "tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "im, class_oh = ld_train[139]\n",
    "print(im.shape)\n",
    "print(im)\n",
    "print(class_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test = Lung_Dataset('test', verbose = 1)\n",
    "ld_val = Lung_Dataset('val', verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size value to be used (to be decided freely, but set to 4 for demo)\n",
    "bs_val = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(ld_train, batch_size = bs_val, shuffle = True)\n",
    "test_loader = DataLoader(ld_test, batch_size = bs_val, shuffle = True)\n",
    "val_loader = DataLoader(ld_val, batch_size = bs_val, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "0\n",
      "tensor([[[[0.0588, 0.0745, 0.1059,  ..., 0.3804, 0.3608, 0.3451],\n",
      "          [0.0510, 0.0706, 0.1020,  ..., 0.3725, 0.3451, 0.3294],\n",
      "          [0.0431, 0.0627, 0.0941,  ..., 0.3608, 0.3294, 0.3098],\n",
      "          ...,\n",
      "          [0.0000, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5608, 0.5569, 0.5294,  ..., 0.3020, 0.2980, 0.2980],\n",
      "          [0.5608, 0.5725, 0.5569,  ..., 0.3098, 0.3059, 0.3059],\n",
      "          [0.5451, 0.5647, 0.5608,  ..., 0.3176, 0.3137, 0.3098],\n",
      "          ...,\n",
      "          [0.0667, 0.0627, 0.0667,  ..., 0.0196, 0.0235, 0.0235],\n",
      "          [0.0667, 0.0588, 0.0667,  ..., 0.0196, 0.0235, 0.0196],\n",
      "          [0.0627, 0.0588, 0.0667,  ..., 0.0196, 0.0235, 0.0196]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0078],\n",
      "          [0.0000, 0.0157, 0.0000,  ..., 0.0039, 0.0000, 0.0078],\n",
      "          [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "          ...,\n",
      "          [0.0039, 0.0000, 0.0118,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0039, 0.0000, 0.0118,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0078, 0.0000, 0.0118,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.1765, 0.2392, 0.2588,  ..., 0.1725, 0.1608, 0.1333],\n",
      "          [0.2039, 0.2667, 0.2941,  ..., 0.3137, 0.2980, 0.2353],\n",
      "          [0.2392, 0.3098, 0.3412,  ..., 0.5098, 0.4706, 0.4118],\n",
      "          ...,\n",
      "          [0.0353, 0.0353, 0.0353,  ..., 0.0431, 0.0431, 0.0431],\n",
      "          [0.0353, 0.0353, 0.0353,  ..., 0.0431, 0.0431, 0.0431],\n",
      "          [0.0353, 0.0353, 0.0353,  ..., 0.0392, 0.0392, 0.0392]]],\n",
      "\n",
      "\n",
      "        [[[0.1843, 0.1765, 0.2353,  ..., 0.2392, 0.1882, 0.1647],\n",
      "          [0.2039, 0.1922, 0.2196,  ..., 0.2863, 0.2235, 0.1843],\n",
      "          [0.2157, 0.2314, 0.2353,  ..., 0.3373, 0.2706, 0.2275],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1137, 0.1098, 0.1020,  ..., 0.0392, 0.0196, 0.0588],\n",
      "          [0.1137, 0.1059, 0.1020,  ..., 0.0471, 0.0471, 0.0706],\n",
      "          [0.1137, 0.1059, 0.0980,  ..., 0.0510, 0.0549, 0.0549],\n",
      "          ...,\n",
      "          [0.1294, 0.1255, 0.1176,  ..., 0.0824, 0.0824, 0.0667],\n",
      "          [0.1412, 0.1255, 0.1098,  ..., 0.0863, 0.0863, 0.0863],\n",
      "          [0.1490, 0.1216, 0.1059,  ..., 0.0863, 0.0824, 0.1176]]]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Typical mini-batch for loop on dataloader (train)\n",
    "for k, v in enumerate(train_loader):\n",
    "    print(\"-----\")\n",
    "    print(k)\n",
    "    print(v[0])\n",
    "    print(v[1])\n",
    "    # Forced stop\n",
    "    break\n",
    "    #assert False, \"Forced stop after one iteration of the for loop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Conv2D: 1 input channel, 8 output channels, 3 by 3 kernel, stride of 1.\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
    "        self.fc1 = nn.Linear(87616, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim = 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7334, -0.6544],\n",
      "        [-0.7805, -0.6128],\n",
      "        [-0.7904, -0.6045],\n",
      "        [-0.7848, -0.6092],\n",
      "        [-0.7673, -0.6241],\n",
      "        [-0.7157, -0.6711],\n",
      "        [-0.6653, -0.7218],\n",
      "        [-0.8203, -0.5803],\n",
      "        [-0.8318, -0.5714],\n",
      "        [-0.7809, -0.6125],\n",
      "        [-0.7664, -0.6249],\n",
      "        [-0.7603, -0.6302],\n",
      "        [-0.7708, -0.6211],\n",
      "        [-0.7361, -0.6519],\n",
      "        [-0.7718, -0.6202],\n",
      "        [-0.7835, -0.6103],\n",
      "        [-0.7147, -0.6721],\n",
      "        [-0.7311, -0.6566],\n",
      "        [-0.7478, -0.6413],\n",
      "        [-0.7233, -0.6639],\n",
      "        [-0.7212, -0.6659],\n",
      "        [-0.6976, -0.6887],\n",
      "        [-0.7867, -0.6076],\n",
      "        [-0.7352, -0.6528],\n",
      "        [-0.7684, -0.6232],\n",
      "        [-0.8197, -0.5808],\n",
      "        [-0.7811, -0.6123],\n",
      "        [-0.8205, -0.5802],\n",
      "        [-0.7506, -0.6388],\n",
      "        [-0.7980, -0.5983],\n",
      "        [-0.7653, -0.6259],\n",
      "        [-0.7089, -0.6777]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Try model on one mini-batch\n",
    "for batch_idx, (images_data, target_labels) in enumerate(train_loader):\n",
    "    predicted_labels = model(images_data)\n",
    "    print(predicted_labels)\n",
    "    print(target_labels)\n",
    "    # Forced stop\n",
    "    break\n",
    "    #assert False, \"Forced stop after one iteration of the mini-batch for loop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
